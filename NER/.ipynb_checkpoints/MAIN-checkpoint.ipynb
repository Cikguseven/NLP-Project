{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04425eae-ee72-416d-8013-4ca400576266",
   "metadata": {},
   "source": [
    "Scrape r/sg subreddit for comments in popular posts from different time filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4dc77-65d3-4f94-8dd2-636abe69fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "reddit = praw.Reddit(client_id='KQpseqhxhSGWZ0CXQrdCkQ',\n",
    "                     client_secret='Xsbo93sniLTYdisbEqlIoBzTNpZDFA', user_agent='MyBot/0.0.1')\n",
    "\n",
    "subreddit = reddit.subreddit('Singapore')\n",
    "\n",
    "time_filters = ['week', 'month', 'year', 'all']\n",
    "\n",
    "viewed_posts = []\n",
    "\n",
    "f = open(\"redditsg.txt\", \"w\", encoding='utf-8')\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Scrapes authors and comments in r/sg subreddit.\n",
    "for filter in time_filters:\n",
    "    for post in subreddit.top(filter, limit=20):\n",
    "        if post.id not in viewed_posts:\n",
    "            print(f'{filter}|{post.id}')\n",
    "            viewed_posts.append(post.id)\n",
    "            submission = reddit.submission(id=post.id)\n",
    "            submission.comments.replace_more(limit=0)\n",
    "\n",
    "            for comment in submission.comments.list():\n",
    "                x = comment.body.replace(\"\\n\",\" \")\n",
    "                f.write(f\"{comment.author}, {x}\\n\")\n",
    "                counter += 1\n",
    "            \n",
    "f.close()\n",
    "\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(f'Time taken: {duration} seconds')\n",
    "print(f'Number of comments scrapped: {counter}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea16dd8-e87a-4ead-8dab-3b5abf38460c",
   "metadata": {},
   "source": [
    "Filter comments based on length and potential quality/value of content. Clean up comments to desired format using RegEx by replacing certain short forms and lingos with proper english words and abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65097503-45ff-48a5-a85d-e70f481316ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 4.88 seconds\n",
      "Number of filtered comments: 9514\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "import re\n",
    "from filters import bots, regex_replacements\n",
    "\n",
    "f = open(\"redditsg.txt\", \"r\", encoding='utf-8')\n",
    "nf = open(\"redditsg_clean.txt\", \"w\")\n",
    "\n",
    "counter_0 = 0\n",
    "\n",
    "for x in sorted(f):\n",
    "    split = x.find(\",\")\n",
    "    if x[:split] not in bots:\n",
    "        x = x[split + 2:]\n",
    "        if 15 <= len(x.split()) and re.search('[a-zA-Z]', x):\n",
    "            for old, new in regex_replacements:\n",
    "                x = re.sub(old, new, x, flags=re.I)\n",
    "\n",
    "            # x = x.lower()\n",
    "            x = x.strip()\n",
    "\n",
    "            nf.write(f\"{x}\\n\")\n",
    "            counter_0 += 1\n",
    "        \n",
    "f.close()\n",
    "nf.close()\n",
    "\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(f'Time taken: {duration} seconds')\n",
    "print(f'Number of filtered comments: {counter_0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efacbbb-4bb3-460d-93b3-e76da6f3d294",
   "metadata": {},
   "source": [
    "Run Spacy's default pipeline on comments to obtain Doc objects. Refer to https://spacy.io/usage/processing-pipelines for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f36ad92f-23ba-4987-bc46-a9dbdf3eb5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 66.31 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import copy\n",
    "import spacy\n",
    "import skweak\n",
    "\n",
    "spacy.require_gpu()\n",
    "\n",
    "f = open(\"redditsg_clean.txt\", \"r\")\n",
    "\n",
    "texts = {comment for comment in f}\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "docs = list(nlp.pipe(texts))\n",
    "\n",
    "# for doc in docs:\n",
    "#     skweak.utils.display_entities(doc)\n",
    "    \n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(f'Time taken: {duration} seconds')\n",
    "print(f'Number of filtered comments: {len(texts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746872ea-824e-413e-af6d-a6f596787e32",
   "metadata": {
    "tags": []
   },
   "source": [
    "Applying default company detector function to Doc objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ff97e-c0df-4491-9a8e-d26a380068b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "start = time.time()\n",
    "\n",
    "def company_detector_fun(doc):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if chunk[-1].lower_.rstrip(\".\") in {'corp', 'inc', 'ltd', 'llc', 'sa', 'ag', 'bhd'}:\n",
    "            yield chunk.start, chunk.end, \"COMPANY\"\n",
    "\n",
    "company_detector = skweak.heuristics.FunctionAnnotator(\n",
    "    \"company_detector\", company_detector_fun)\n",
    "\n",
    "# We run the function on the full corpus\n",
    "docs = list(company_detector.pipe(docs))\n",
    "\n",
    "for doc in docs:\n",
    "    skweak.utils.display_entities(doc, \"company_detector\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "duration = round(end - start, 2)\n",
    "\n",
    "print(f'Time taken: {duration}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf6b90-6656-40df-ba8d-860d100b20fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Applying default other organisations detector function to Doc objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16948ac8-0624-498e-a342-9aef853c9e6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "start = time.time()\n",
    "\n",
    "OTHER_ORG_CUE_WORDS = {\"University\", \"Institute\", \"College\", \"Committee\",\n",
    "                        \"Party\", \"Agency\", \"Union\", \"Association\",\n",
    "                        \"Organization\", \"Court\", \"Office\", \"National\"}\n",
    "\n",
    "def other_org_detector_fun(doc):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if any([tok.text in OTHER_ORG_CUE_WORDS for tok in chunk]):\n",
    "            yield chunk.start, chunk.end, \"OTHER_ORG\"\n",
    "\n",
    "# We create the labelling function\n",
    "other_org_detector = skweak.heuristics.FunctionAnnotator(\n",
    "    \"other_org_detector\", other_org_detector_fun)\n",
    "\n",
    "# We run the function on the full corpus\n",
    "docs = list(other_org_detector.pipe(docs))\n",
    "\n",
    "for doc in docs:\n",
    "    skweak.utils.display_entities(doc, \"other_org_detector\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "duration = round(end - start, 2)\n",
    "\n",
    "print(f'Time taken: {duration}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f842a6f-3720-4870-a488-41b412156bf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Trying out heuristic labelling function (LF) on COVID and healthcare related terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4d955-775a-4204-a3aa-2a806e961354",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "start = time.time()\n",
    "\n",
    "COVID_CUE_WORDS = {\"covid\", \"doctor\", \"doctors\", \"dr\", \"swab\", \"nurse\", \"nurses\", \"icu\", \"a&e\", \"virus\", \"healthcare\", \"hospital\", \"wards\", \"pandemic\", \"vaccination\", \"booster\", \"masks\"}\n",
    "\n",
    "def covid_detector_fun(doc):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if any([tok.text.lower() in COVID_CUE_WORDS for tok in chunk]):\n",
    "            yield chunk.start, chunk.end, \"Healthcare\"\n",
    "\n",
    "# We create the labelling function\n",
    "covid_detector = skweak.heuristics.FunctionAnnotator(\n",
    "    \"covid_detector\", covid_detector_fun)\n",
    "\n",
    "# We run the function on the full corpus\n",
    "docs = list(covid_detector.pipe(docs))\n",
    "\n",
    "# Showing some examples\n",
    "for doc in docs:\n",
    "    skweak.utils.display_entities(doc, \"covid_detector\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "duration = round(end - start, 2)\n",
    "\n",
    "print(f'Time taken: {duration}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da89919-bf05-4fb4-9cfc-afc999efbfbe",
   "metadata": {},
   "source": [
    "Trying out heuristic labelling function to detect locations in SG (MRT and LRT stations/places of interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc17b89c-90c5-4f40-a894-fe87ba829d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bakau', 'bangkit', 'bukit panjang', 'cheng lim', 'choa chu kang', 'compassvale', 'coral edge', 'cove', 'damai', 'fajar', 'farmway', 'fernvale', 'jelapang', 'kadaloor', 'kangkar', 'keat hong', 'kupang', 'layar', 'meridian', 'nibong', 'oasis', 'pending', 'petir', 'phoenix', 'punggol', 'punggol point', 'ranggung', 'renjong', 'riviera', 'rumbia', 'sam kee', 'samudera', 'segar', 'sengkang', 'senja', 'soo teck', 'south view', 'sumang', 'teck lee', 'teck whye', 'ten mile junction', 'thanggam', 'tongkang']\n",
      "Time taken: 1.51 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "from sg_locations import *\n",
    "\n",
    "def sg_loc_fun(doc):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        uncased = chunk.text.lower()\n",
    "        if ('mrt' in uncased and uncased.replace(' mrt', '').replace(' station', '') in mrt_stations\n",
    "                or 'lrt' in uncased and uncased.replace(' lrt', '').replace(' station', '') in lrt_stations or uncased in aggregated_locations):\n",
    "            yield chunk.start, chunk.end, \"LOC\"\n",
    "\n",
    "# We create the labelling function\n",
    "sg_loc_detector = skweak.heuristics.FunctionAnnotator(\"sg_loc_detector\", sg_loc_fun)\n",
    "\n",
    "# We run the function on the full corpus\n",
    "docs = list(sg_loc_detector.pipe(docs))\n",
    "\n",
    "# for doc in docs:\n",
    "#     skweak.utils.display_entities(doc, \"sg_loc_detector\")\n",
    "    \n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(f'Time taken: {duration} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d73b2e-3968-45d6-96d6-a3a6cb0f7c86",
   "metadata": {
    "tags": []
   },
   "source": [
    "Applying annotations from Spacy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be9a503-5ce3-4835-8b45-6ad754bba6f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "start = time.time()\n",
    "\n",
    "annotator = skweak.spacy.ModelAnnotator(\"conll2003\", \"data/conll2003\")\n",
    "\n",
    "docs = list(annotator.pipe(docs))\n",
    "\n",
    "for doc in docs:\n",
    "    skweak.utils.display_entities(doc, \"conll2003\")\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "duration = round(end - start, 2)\n",
    "\n",
    "print(f'Time taken: {duration} seconds')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f5bdc-ee94-4f6f-9e05-d0dd53a7bb4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Applying gazetteers to search for entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "806c08c3-3be2-4a2b-8163-a77f691ae5ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from data/crunchbase.json\n",
      "Populating trie for class COMPANY (number: 788714)\n",
      "Populating trie for class ORG (number: 261)\n",
      "Populating trie for class PERSON (number: 1062669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The companies listed are: \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    shopee\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    lazada\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       ", ntuc, \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Taobao\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Grab\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    gojek\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    osim\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    dbs bank\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Singapore Airlines\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       ", capitaland. In addition, \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tata\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       " also \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    wins\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       " the award.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 10.92 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import spacy, skweak\n",
    "\n",
    "tries = skweak.gazetteers.extract_json_data(\"data/crunchbase.json\")\n",
    "gazetteer = skweak.gazetteers.GazetteerAnnotator(\"gazetteer\", tries, case_sensitive=False)\n",
    "\n",
    "# We run the function on the full corpus\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp('The companies listed are: shopee, lazada, ntuc, Taobao, Grab, gojek, osim, dbs bank, Singapore Airlines, capitaland. In addition, tata also wins the award.')\n",
    "gazetteer(doc)\n",
    "skweak.utils.display_entities(doc, \"gazetteer\")\n",
    "\n",
    "# docs = list(gazetteer.pipe(docs))\n",
    "\n",
    "# Showing some examples\n",
    "# for doc in docs[:10]:\n",
    "#     skweak.utils.display_entities(doc, \"gazetteer\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "duration = round(end - start, 2)\n",
    "\n",
    "print(f'Time taken: {duration} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9883c1-b467-4851-8dca-389490fbac3e",
   "metadata": {},
   "source": [
    "Implementation of combined annotators from conll2003_ner.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc342d4-b083-4234-ab93-c0d4c21a5614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shallow functions\n",
      "Loading Spacy NER models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiero\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\util.py:833: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.2.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gazetteer supervision modules\n",
      "Extracting data from C:\\Users\\kiero\\Documents\\GitHub\\NER-Project/data/wikidata_tokenised.json\n",
      "Populating trie for class PERSON (number: 2621131)\n",
      "Populating trie for class LOC (number: 47104)\n",
      "Populating trie for class GPE (number: 601419)\n",
      "Populating trie for class ORG (number: 295449)\n",
      "Populating trie for class PRODUCT (number: 12457)\n",
      "Extracting data from C:\\Users\\kiero\\Documents\\GitHub\\NER-Project/data/wikidata_small_tokenised.json\n",
      "Populating trie for class PERSON (number: 1863434)\n",
      "Populating trie for class LOC (number: 14241)\n",
      "Populating trie for class GPE (number: 273373)\n",
      "Populating trie for class ORG (number: 91341)\n",
      "Populating trie for class PRODUCT (number: 12457)\n",
      "Extracting data from C:\\Users\\kiero\\Documents\\GitHub\\NER-Project/data/geonames.json\n",
      "Populating trie for class GPE (number: 15205)\n",
      "Extracting data from C:\\Users\\kiero\\Documents\\GitHub\\NER-Project/data/crunchbase.json\n",
      "Populating trie for class COMPANY (number: 788714)\n",
      "Populating trie for class ORG (number: 261)\n",
      "Populating trie for class PERSON (number: 1062669)\n",
      "Extracting data from C:\\Users\\kiero\\Documents\\GitHub\\NER-Project/data/products.json\n",
      "Populating trie for class PRODUCT (number: 45362)\n",
      "Loading document-level supervision sources\n",
      "Total number of annotators: 52\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "import conll2003_ner\n",
    "\n",
    "full_annotator = conll2003_ner.NERAnnotator().add_all()\n",
    "print(\"Total number of annotators:\", len(full_annotator.annotators))\n",
    "\n",
    "docs = list(full_annotator.pipe(docs))\n",
    "\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(f'Time taken: {duration} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ecbf8-f67c-4320-a8d2-2db889193dc2",
   "metadata": {},
   "source": [
    "Resolve aggregated labelling functions to create a single annotation for each document by estimating a generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a074df70-6480-4b02-9532-3a9c362bbce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "unified_model = skweak.aggregation.HMM(\"hmm\", [\"LOC\", \"MISC\", \"ORG\", \"PER\"])\n",
    "unified_model.add_underspecified_label(\"ENT\", [\"LOC\", \"MISC\", \"ORG\", \"PER\"])\n",
    "\n",
    "# We then run Baum-Welch on the model (can take some time)\n",
    "docs = unified_model.fit_and_aggregate(docs)\n",
    "\n",
    "# unified_model.pretty_print() \n",
    "\n",
    "# unified_model.save(\"hmm_reddit.pkl\")\n",
    "\n",
    "for doc in docs[:5]:\n",
    "    skweak.utils.display_entities(doc, \"hmm\") \n",
    "    \n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(f'Time taken: {duration} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69eac42-4d63-4341-970d-128ff78fc413",
   "metadata": {},
   "source": [
    "Directly train a new NER model with Spacy's training regime based on annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac2215-eddf-4b3c-bbd6-2d7cc8382fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    doc.ents = doc.spans[\"hmm\"]\n",
    "skweak.utils.docbin_writer(docs, \"reddit.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df37ff-9f4d-40a8-bb74-1126cb4373ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spacy init config - --lang en --pipeline ner --optimize accuracy | \\\n",
    "spacy train - --paths.train reddit.spacy  --paths.dev reddit.spacy \\\n",
    "--initialize.vectors en_core_web_md --output reddit_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe4719-bad6-4d81-9eae-6846f25397fa",
   "metadata": {},
   "source": [
    "Load the learned model and run it on chosen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9706e08-9057-4cd6-9edc-8bc343e9efd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1.36 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import spacy\n",
    "import skweak\n",
    "import csv\n",
    "\n",
    "spacy.require_gpu()\n",
    "\n",
    "f = open(\"mrt.txt\", \"r\")\n",
    "texts = [comment for comment in f]\n",
    "\n",
    "nlp = spacy.load(\"reddit_output/model-best\")\n",
    "test_docs = list(nlp.pipe(texts))\n",
    "\n",
    "entities = []\n",
    "\n",
    "with open('mrt_ents.csv','w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['text','label'])\n",
    "    for doc in test_docs:\n",
    "        # skweak.utils.display_entities(doc)\n",
    "        for ent in doc.ents:\n",
    "            row = (ent.text, ent.label_)\n",
    "            if row not in entities:\n",
    "                csv_out.writerow(row)\n",
    "                entities.append(row)\n",
    "    \n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(f'Time taken: {duration} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114d713-668f-4400-b3fe-30ed13e8379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('reddit_ents.csv','w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['text','label'])\n",
    "    for entry in entities:\n",
    "        csv_out.writerow(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70277050-2043-4cf4-b5f5-e6e5503f8cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
